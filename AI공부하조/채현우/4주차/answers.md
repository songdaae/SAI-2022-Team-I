# 4ì£¼ì°¨ íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜

## ì´ë¡ ë¬¸ì œ 1
**ë‹¤ìŒ ì¤‘ ê²°ì • íŠ¸ë¦¬ì— ê´€í•œ ì§ˆë¬¸ ì¤‘ í‹€ë¦° ê²ƒì„ ëª¨ë‘ ê³ ë¥´ì‹œì˜¤.**
**ë‹µ: 3, 5**

	1.  ê²°ì • íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜ì€ ë¶€ëª¨ì™€ ìì‹ ë…¸ë“œ ì‚¬ì´ì˜ ë¶ˆìˆœë„ ì°¨ì´ê°€ ìµœëŒ€í™”ë˜ë„ë¡ í•™ìŠµí•œë‹¤.
	2.  ê²°ì •íŠ¸ë¦¬ì— ì•„ë¬´ íŒŒë¼ë¯¸í„°ë„ ì£¼ì§€ ì•Šì€ ì±„ í•™ìŠµí•˜ë©´ ì˜¤ë²„í”¼íŒ…ëœë‹¤
	3.  ì´ì§„ ë¶„ë¥˜ì—ì„œ ì§€ë‹ˆ ë¶ˆìˆœë„ì˜ ìµœëŒ€ ê°’ì€ 0.5ì´ê³  í´ìˆ˜ë¡ ë¶„ë¥˜í•˜ê¸° ì¢‹ì§€ ì•Šê³ , ì—”íŠ¸ë¡œí”¼ ë¶ˆìˆœë„ëŠ” ë‚®ì„ìˆ˜ë¡ ë¶„ë¥˜í•˜ê¸° ì¢‹ì§€ ì•Šë‹¤.
	4.  ê²°ì •íŠ¸ë¦¬ì—ì„œ íŠ¹ì„± ì¤‘ìš”ë„ëŠ” ë¶„ë¥˜ë¥¼ ìˆœìˆ˜í•˜ê²Œ ì˜ í•œ ê¸°ì—¬ë„ì´ê³ , íŠ¹ì„± ì¤‘ìš”ë„ê°€ ë†’ì„ ìˆ˜ë¡ í•´ë‹¹ ë…¸ë“œê°€ ì¤‘ìš”í•œ ë¶„ë¥˜ ê¸°ì¤€ì´ë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.
	5.  íŠ¹ì„± ì¤‘ìš”ë„ê°€ ë‚®ë‹¤ëŠ” ê²ƒì€ ìœ ìš©í•˜ì§€ ì•Šì€ íŠ¹ì„±ì´ë‹¤. **- ê²°ì •ì— ë§ì€ ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤?**
3. ë¶ˆìˆœë„ëŠ” ì–‘ì„±ë°ì´í„°ì™€ ìŒì„±ë°ì´í„°ì˜ ê°œìˆ˜ì°¨ì´ê°€ ì–¼ë§ˆë‚˜ ë‚˜ëŠ”ì§€ë¥¼ ìˆ˜ì¹˜í™” í•œ ê²ƒìœ¼ë¡œ **ë¶„ë¥˜í•˜ê¸° ì¢‹ì€ ê²ƒ** ë³´ë‹¤ëŠ” **ë¶„ë¥˜ê°€ ì˜ ëœ ê²ƒ**ì´ë¼ í•˜ëŠ”ê²Œ ë§ëŠ” ê²ƒ ê°™ë‹¤. ë˜í•œ ì—”íŠ¸ë¡œí”¼ ë¶ˆìˆœë„ ë˜í•œ **ë‚®ì„ ìˆ˜ë¡ ë¶„ë¥˜ê°€ ì˜ ëœ ê²ƒ**ì´ë‹¤. 
5. ë‹¨ì§€ ê·¸ íŠ¹ì„±ì„ ì„ íƒí•˜ì§€ ì•Šì•˜ì„ ë¿, ë‹¤ë¥¸ íŠ¹ì„±ì´ ë™ì¼í•œ ì •ë³´ë¥¼ ê°€ì ¸ì„œ ì¼ ìˆ˜ë„ ìˆë‹¤.

## ì´ë¡  ë¬¸ì œ 2
**ë‹¤ìŒ ë‚´ìš© ì¤‘ í‹€ë¦° ì„¤ëª…ì„ ëª¨ë‘ ê³ ë¥´ì‹œì˜¤.**
**ë‹µ: 3, 4**

	1.  ì§‘ë‹¨ì˜ ë°ì´í„° ê°œìˆ˜ë§Œí¼ì„ ë³µì› ì¶”ì¶œí•˜ëŠ” bootstrapì€ ëª¨ì§‘ë‹¨ê³¼ í‘œë³¸ ì§‘ë‹¨ì„ ì¶”ì • ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.
	2.  Histogram-based Gradient Boostingì€ ì •í˜• ë°ì´í„°ë¥¼ ë†’ì€ ì„±ëŠ¥ìœ¼ë¡œ ë‹¤ë£¬ë‹¤.
	3.  íŠ¸ë¦¬ì˜ ëœë¤ì„±ì´ í´ìˆ˜ë¡ ë°©ëŒ€í•œ ì–‘ì˜ íŠ¸ë¦¬ë¥¼ í›ˆë ¨í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— í•„ì—°ì ìœ¼ë¡œ ê³„ì‚° ì†ë„ê°€ ëŠë ¤ì§€ëŠ” ë¬¸ì œê°€ ë°œìƒí•œë‹¤.
	4.  random forestì—ì„œ ê°ê°ì˜ íŠ¸ë¦¬ëŠ” ì˜¤ë²„í”¼íŒ…ë  ìˆ˜ ìˆìœ¼ë‚˜ ê° íŠ¸ë¦¬ì˜ ì—°ê´€ì„±ì´ ë†’ì„ìˆ˜ë¡ random forestì˜ ì„±ëŠ¥ì´ ë†’ì•„ì§„ë‹¤.
	5.  íšŒê·€ì™€ ë¶„ë¥˜ ëª¨ë‘ì— ì´ìš© ê°€ëŠ¥í•œ gradient boostingì€ ì†ì‹¤ í•¨ìˆ˜, ì•½í•œ í•™ìŠµìë“¤ì„ í¬í•¨í•œë‹¤.

3. íŠ¸ë¦¬ì˜ ëœë¤ì„±ì´ í¬ë©´ ì„±ëŠ¥ì´ ì¢‹ì•„ì§‘ë‹ˆë‹¤. ëŒ€í‘œì ì¸ ì˜ˆë¡œ ì—‘ìŠ¤íŠ¸ë¼ íŠ¸ë¦¬
4. Random ForestëŠ” ê³¼ëŒ€ì í•©ì´ ìˆëŠ” íŠ¸ë¦¬ê°€ ìˆìœ¼ë©´, ë‹¤ë¥¸ ìª½ìœ¼ë¡œ ê³¼ëŒ€ì í•©ì´ ë˜ê²Œ í•˜ì—¬, ê³¼ëŒ€ì í•©ì„ ë§‰ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë¯€ë¡œ, íŠ¸ë¦¬ì˜ ì—°ê´€ì„±ì´ ì—†ì„ ìˆ˜ë¡ ì„±ëŠ¥ì´ ë†’ì•„ì§„ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.

## ì‹¤ìŠµ ë¬¸ì œ 3
**GridSearchCVë¥¼ ì•Œê²Œëœ í˜¼ê³µë¨¸ì‹ ì€ 04-1ì—ì„œ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ ì´ìš©í•˜ì—¬ ìƒì„ ì˜ ì¢…ì„ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì˜ Cê°’ì´ ì ì ˆí•œ ê°’ì¸ì§€ ê¶ê¸ˆí•´ì¡Œë‹¤. ê¸°ì¡´ì˜ ì½”ë“œì—ì„œ GridSearchCVë¥¼ ì¶”ê°€í•˜ì—¬ Cê°’ì´ ë³€í™”í•˜ì˜€ëŠ”ì§€ í™•ì¸í•´ë³´ì.**

```python
import pandas as pd

fish = pd.read_csv('<https://bit.ly/fish_csv_data>')

fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()

#data split
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)

#preprocessing
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

#LR
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)

print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))

```

**ì¶œë ¥**

GridSearchCVë¥¼ ì ìš©í•˜ê¸° ì „ê³¼ ì ìš©í•œ í›„ì˜ train, testì˜ scoreë¥¼ ì¶œë ¥í•˜ê³ , GridSearchCVê°€ ì°¾ì€ Cê°’ì´ ê¸°ì¡´ì˜ Cê°’ê³¼ ë‹¤ë¥¸ì§€ ì¶œë ¥í•´ì£¼ì„¸ìš”.

```python
from sklearn.model_selection import GridSearchCV
import numpy as np

params = {'C' : np.arange(1, 200)}
gs = GridSearchCV(LogisticRegression(max_iter=1000), params, n_jobs=-1)
gs.fit(train_scaled, train_target)
bestLR = gs.best_estimator_
print(bestLR.score(train_scaled, train_target))
print(bestLR.score(test_scaled, test_target))
print("Best Value C =", gs.best_params_['C'])

```

![](raw/answer01.png)

`ğŸ ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” Cê°’ì´ ì‘ì„ ìˆ˜ë¡ ê·œì œê°€ ì‹¬í•´ì§€ë¯€ë¡œ ìµœì ì˜ ê°’ì„ ì°¾ì„ ë•Œ Cê°’ì˜ ë²”ìœ„ë¥¼ ì¤„ì—¬ì•¼ í•¨`

## ì´ë¡  ë¬¸ì œ 4
**ë‹¤ìŒ ì¤‘ ê²°ì • íŠ¸ë¦¬ì— ê´€í•œ ì§ˆë¬¸ ì¤‘ í‹€ë¦° ê²ƒì„ ëª¨ë‘ ê³ ë¥´ì‹œì˜¤.**
 **ë‹µ: 2, 3, 6**

	1.  8-ê²¹ êµì°¨ ê²€ì¦ì„ ì‚¬ìš©í•  ë•Œ í›ˆë ¨ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ëŠ” ì „ì²´ ë°ì´í„°ì˜ 87.5%ì´ë‹¤.
	2.  ëª¨ë¸ì—ì„œ ê°ê°ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ë…ë¦½ì ìœ¼ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤.
	3.  Leave One Out Cross Validationì€ Leanear regressionê³¼ polynomial regression ëª¨ë¸ì—ì„œ ë” ëŠë¦° ì—°ì‚° ì†ë„ë¥¼ ê°€ì§„ë‹¤.
	4.  K-fold cross validationì€ LOO cross validationì— ë¹„í•´ ì—°ì‚°ëŸ‰ì´ ì ë‹¤.
	5.  Validation set ApproachëŠ” LOO cross validationì— ë¹„í•´ biasê°€ ë†’ë‹¤.
	6.  LOO cross validationëŠ” ëª¨ë“  íŠ¹ì„±ì„ validationìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— Varianceì€ ë‚®ë‹¤.

2. ë…ë¦½ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë„ ìˆì§€ë§Œ, ì¢…ì†ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë„ ìˆë‹¤
3. êµì°¨ ê²€ì¦ ì—°ì‚° ì†ë„ = ëª¨ë¸ì˜ ì†ë„
6. LOOCVëŠ” ê³¼ëŒ€ì í•©ë˜ëŠ” ê²½í–¥ì´ ìˆìœ¼ë¯€ë¡œ, Varianceì€ ë†’ë‹¤.

## ì‹¤ìŠµ ë¬¸ì œ 5

í˜¼ê³µì´ëŠ” ë°¤í•˜ëŠ˜ì„ ë³´ë˜ ì™€ì¤‘ì— ê°‘ìê¸° í–‰ì„±ì˜ ì¸¡ì • ë°©ë²•ì´ ê¶ê¸ˆí•´ì¡Œë‹¤. ì–´ë–¤ í–‰ì„±ì„ ì–´ë–¤ ë°©ë²•ìœ¼ë¡œ ì¸¡ì •í• ì§€ í•œë²ˆ ì•Œì•„ë³´ì. í˜¼ê³µì´ëŠ” ì˜ˆì¸¡ ê³¼ì •ì„ ì´í•´í•˜ê¸° ì‰½ê²Œ ê²°ì • íŠ¸ë¦¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸°ë¡œ ê²°ì •í–ˆë‹¤. (ë‹¨, Grid SearchëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.)


```python
<ì‹œì‘ ì½”ë“œ>
from sklearn import datasets
import pandas as pd
import seaborn as sns
planets=sns.load_dataset('planets')

```

----------

**ì¶œë ¥**

0.9939613526570048 0.9371980676328503

ë‹¤ìŒê³¼ ê°™ì€ í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ scoreê°’ê³¼ ê²°ì •íŠ¸ë¦¬ ê·¸ë˜í”„ë¥¼ ì¶œë ¥í•´ì£¼ì„¸ìš”.

```python
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
RAND = 81

planets = planets.fillna(0)
planets_input = planets[['number', 'orbital_period', 'mass', 'distance', 'year']]
planets_target = planets['method']

train_input, test_input, train_target, test_target = train_test_split(planets_input, planets_target, random_state=RAND)

dt = DecisionTreeClassifier(random_state=RAND)
dt.fit(train_input, train_target)
print(dt.score(train_input, train_target))
print(dt.score(test_input, test_target))

plot_tree(dt)
plt.show()

```

![](raw/answer02.png)
![](raw/answer03.png)

`ê³¼ëŒ€ì í•©ì„ ì¤„ì´ëŠ” ê²Œ ì¤‘ìš”í•˜ë‹¤`